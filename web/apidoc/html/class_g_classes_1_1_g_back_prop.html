<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>GClasses: GClasses::GBackProp Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript">
$(document).ready(initResizable);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.7.3 -->
<div id="top">
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">GClasses</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
</div>
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
  initNavTree('class_g_classes_1_1_g_back_prop.html','');
</script>
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a> &#124;
<a href="#friends">Friends</a>  </div>
  <div class="headertitle">
<h1>GClasses::GBackProp Class Reference</h1>  </div>
</div>
<div class="contents">
<!-- doxytag: class="GClasses::GBackProp" -->
<p>This class performs backpropagation on a neural network. (I made it a separate class because it is only needed during training. There is no reason to waste this space after training is complete, or if you choose to use a different technique to train the neural network.)  
<a href="#_details">More...</a></p>

<p><code>#include &lt;GNeuralNet.h&gt;</code></p>

<p><a href="class_g_classes_1_1_g_back_prop-members.html">List of all members.</a></p>
<table class="memberdecls">
<tr><td colspan="2"><h2><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#adefd54b49a91ee4abf6215a20cb74f5d">GBackProp</a> (<a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *pNN)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This class will adjust the weights in pNN.  <a href="#adefd54b49a91ee4abf6215a20cb74f5d"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a2fdca5057035e0cd9ee2ee238d2d47aa">~GBackProp</a> ()</td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a84669ea9fd994062855d647248ad4a4e">layer</a> (size_t layer)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns a layer (not a layer of the neural network, but a corresponding layer of values used for back-prop)  <a href="#a84669ea9fd994062855d647248ad4a4e"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a20f93ce50af7554ad32c3b4c703c2316">backPropFromSingleNode</a> (<a class="el" href="class_g_classes_1_1_g_neuron.html">GNeuron</a> &amp;nnFrom, <a class="el" href="class_g_classes_1_1_g_back_prop_neuron.html">GBackPropNeuron</a> &amp;bpFrom, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNToLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPToLayer)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Backpropagates the error from a single output node to a hidden layer.  <a href="#a20f93ce50af7554ad32c3b4c703c2316"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a385180d0de33b67b4cac64aebeb8efbb">adjustWeightsSingleNeuron</a> (<a class="el" href="class_g_classes_1_1_g_neuron.html">GNeuron</a> &amp;nnFrom, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNToLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_neuron.html">GBackPropNeuron</a> &amp;bpFrom, double learningRate, double momentum)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Adjust the weights of a single neuron that follows a hidden layer. (Assumes the error of this neuron has already been computed).  <a href="#a385180d0de33b67b4cac64aebeb8efbb"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a40933a0e157d4e9a479db7a0174934cf">adjustWeightsSingleNeuron</a> (<a class="el" href="class_g_classes_1_1_g_neuron.html">GNeuron</a> &amp;nnFrom, const double *pFeatures, bool useInputBias, <a class="el" href="class_g_classes_1_1_g_back_prop_neuron.html">GBackPropNeuron</a> &amp;bpFrom, double learningRate, double momentum)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Adjust the weights of a single neuron when there are no hidden layers. (Assumes the error of this neuron has already been computed).  <a href="#a40933a0e157d4e9a479db7a0174934cf"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a71d0f533c7ae13baf0780a5ac8aa9e57">backpropagate</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This method assumes that the error term is already set at every unit in the output layer. It uses back-propagation to compute the error term at every hidden unit. (It does not update any weights.)  <a href="#a71d0f533c7ae13baf0780a5ac8aa9e57"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a7c0388ce51ff18b368fd00a32b6a19c5">backpropagateSingleOutput</a> (size_t outputNode)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Backpropagates error from a single output node over all of the hidden layers. (Assumes the error term is already set on the specified output node.)  <a href="#a7c0388ce51ff18b368fd00a32b6a19c5"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a2e9f7e130763bd31e679e6570e66fcf3">descendGradient</a> (const double *pFeatures, double learningRate, double momentum, bool useInputBias)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This method assumes that the error term is already set for every network unit. It adjusts weights to descend the gradient of the error surface with respect to the weights.  <a href="#a2e9f7e130763bd31e679e6570e66fcf3"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a325e169c24a186eaad819eae6e3be0d5">descendGradientSingleOutput</a> (size_t outputNeuron, const double *pFeatures, double learningRate, double momentum, bool useInputBias)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This method assumes that the error term has been set for a single output network unit, and all units that feed into it transitively. It adjusts weights to descend the gradient of the error surface with respect to the weights.  <a href="#a325e169c24a186eaad819eae6e3be0d5"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#ac8e70a4fabc242c6c1db76dbbe8b8f21">adjustFeatures</a> (double *pFeatures, double learningRate, size_t skip, bool useInputBias)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This method assumes that the error term is already set for every network unit. It descends the gradient by adjusting the features (not the weights).  <a href="#ac8e70a4fabc242c6c1db76dbbe8b8f21"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#af94293b82f3a1430d2426b043281d05d">adjustFeaturesSingleOutput</a> (size_t outputNeuron, double *pFeatures, double learningRate, bool useInputBias)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This adjusts the features (not the weights) to descend the gradient, assuming that the error is computed from only one of the output units of the network.  <a href="#af94293b82f3a1430d2426b043281d05d"></a><br/></td></tr>
<tr><td colspan="2"><h2><a name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">static void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a4648bfa8f89eb1d6abd757f7d36491bb">backPropLayer</a> (<a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNToLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPToLayer, size_t fromBegin=0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Backpropagates the error from the "from" layer to the "to" layer. (If the "to" layer has fewer units than the "from" layer, then it will begin propagating with the (fromBegin+1)th weight and stop when the "to" layer runs out of units. It would be an error if the number of units in the "from" layer is less than the number of units in the "to" layer plus fromBegin.  <a href="#a4648bfa8f89eb1d6abd757f7d36491bb"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">static void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a4ffe72bd0d4ae38ffa51afef9f7c5537">backPropLayer2</a> (<a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer1, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer2, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNToLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer1, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer2, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPToLayer, size_t pass)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This is another implementation of backPropLayer. This one is somewhat more flexible, but slightly less efficient. It supports backpropagating error from one or two layers. (pNNFromLayer2 should be NULL if you are backpropagating from just one layer.) It also supports temporal backpropagation by unfolding in time and then averaging the error across all of the unfolded instantiations. "pass" specifies how much of the error for this pass to accept. 1=all of it, 2=half of it, 3=one third, etc.  <a href="#a4ffe72bd0d4ae38ffa51afef9f7c5537"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">static void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a91971cd69d87e8cce0a2d18bc37c3b46">adjustWeights</a> (<a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer, <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNToLayer, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer, double learningRate, double momentum)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Adjust weights in pNNFromLayer. (The error for pNNFromLayer layer must have already been computed.) (If you are backpropagating error from two layers, you can just call this method twice, once for each previous layer.)  <a href="#a91971cd69d87e8cce0a2d18bc37c3b46"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">static void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a9d16161f3687fa651b8a507a9fa4dff5">adjustWeights</a> (<a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *pNNFromLayer, const double *pFeatures, bool useInputBias, <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *pBPFromLayer, double learningRate, double momentum)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Adjust weights in pNNFromLayer. (The error for pNNFromLayer layer must have already been computed.) (If you are backpropagating error from two layers, you can just call this method twice, once for each previous layer.)  <a href="#a9d16161f3687fa651b8a507a9fa4dff5"></a><br/></td></tr>
<tr><td colspan="2"><h2><a name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#af1484d812296d23a0b332942f5bfff73">m_pNN</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">std::vector&lt; <a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a5e01ba2da853068b910c71072459ec0e">m_layers</a></td></tr>
<tr><td colspan="2"><h2><a name="friends"></a>
Friends</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">class&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_back_prop.html#a9311a6752671c2a8e3cf8d8aacff043e">GNeuralNet</a></td></tr>
</table>
<hr/><a name="_details"></a><h2>Detailed Description</h2>
<div class="textblock"><p>This class performs backpropagation on a neural network. (I made it a separate class because it is only needed during training. There is no reason to waste this space after training is complete, or if you choose to use a different technique to train the neural network.) </p>
</div><hr/><h2>Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="adefd54b49a91ee4abf6215a20cb74f5d"></a><!-- doxytag: member="GClasses::GBackProp::GBackProp" ref="adefd54b49a91ee4abf6215a20cb74f5d" args="(GNeuralNet *pNN)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">GClasses::GBackProp::GBackProp </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *&#160;</td>
          <td class="paramname"><em>pNN</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This class will adjust the weights in pNN. </p>

</div>
</div>
<a class="anchor" id="a2fdca5057035e0cd9ee2ee238d2d47aa"></a><!-- doxytag: member="GClasses::GBackProp::~GBackProp" ref="a2fdca5057035e0cd9ee2ee238d2d47aa" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">GClasses::GBackProp::~GBackProp </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<hr/><h2>Member Function Documentation</h2>
<a class="anchor" id="ac8e70a4fabc242c6c1db76dbbe8b8f21"></a><!-- doxytag: member="GClasses::GBackProp::adjustFeatures" ref="ac8e70a4fabc242c6c1db76dbbe8b8f21" args="(double *pFeatures, double learningRate, size_t skip, bool useInputBias)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::adjustFeatures </td>
          <td>(</td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>skip</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>useInputBias</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This method assumes that the error term is already set for every network unit. It descends the gradient by adjusting the features (not the weights). </p>

</div>
</div>
<a class="anchor" id="af94293b82f3a1430d2426b043281d05d"></a><!-- doxytag: member="GClasses::GBackProp::adjustFeaturesSingleOutput" ref="af94293b82f3a1430d2426b043281d05d" args="(size_t outputNeuron, double *pFeatures, double learningRate, bool useInputBias)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::adjustFeaturesSingleOutput </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>outputNeuron</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>useInputBias</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This adjusts the features (not the weights) to descend the gradient, assuming that the error is computed from only one of the output units of the network. </p>

</div>
</div>
<a class="anchor" id="a9d16161f3687fa651b8a507a9fa4dff5"></a><!-- doxytag: member="GClasses::GBackProp::adjustWeights" ref="a9d16161f3687fa651b8a507a9fa4dff5" args="(GNeuralNetLayer *pNNFromLayer, const double *pFeatures, bool useInputBias, GBackPropLayer *pBPFromLayer, double learningRate, double momentum)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">static void GClasses::GBackProp::adjustWeights </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>useInputBias</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&#160;</td>
          <td class="paramname"><em>pBPFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>momentum</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Adjust weights in pNNFromLayer. (The error for pNNFromLayer layer must have already been computed.) (If you are backpropagating error from two layers, you can just call this method twice, once for each previous layer.) </p>

</div>
</div>
<a class="anchor" id="a91971cd69d87e8cce0a2d18bc37c3b46"></a><!-- doxytag: member="GClasses::GBackProp::adjustWeights" ref="a91971cd69d87e8cce0a2d18bc37c3b46" args="(GNeuralNetLayer *pNNFromLayer, GNeuralNetLayer *pNNToLayer, GBackPropLayer *pBPFromLayer, double learningRate, double momentum)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">static void GClasses::GBackProp::adjustWeights </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&#160;</td>
          <td class="paramname"><em>pBPFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>momentum</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Adjust weights in pNNFromLayer. (The error for pNNFromLayer layer must have already been computed.) (If you are backpropagating error from two layers, you can just call this method twice, once for each previous layer.) </p>

</div>
</div>
<a class="anchor" id="a40933a0e157d4e9a479db7a0174934cf"></a><!-- doxytag: member="GClasses::GBackProp::adjustWeightsSingleNeuron" ref="a40933a0e157d4e9a479db7a0174934cf" args="(GNeuron &amp;nnFrom, const double *pFeatures, bool useInputBias, GBackPropNeuron &amp;bpFrom, double learningRate, double momentum)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::adjustWeightsSingleNeuron </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neuron.html">GNeuron</a> &amp;&#160;</td>
          <td class="paramname"><em>nnFrom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>useInputBias</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_neuron.html">GBackPropNeuron</a> &amp;&#160;</td>
          <td class="paramname"><em>bpFrom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>momentum</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Adjust the weights of a single neuron when there are no hidden layers. (Assumes the error of this neuron has already been computed). </p>

</div>
</div>
<a class="anchor" id="a385180d0de33b67b4cac64aebeb8efbb"></a><!-- doxytag: member="GClasses::GBackProp::adjustWeightsSingleNeuron" ref="a385180d0de33b67b4cac64aebeb8efbb" args="(GNeuron &amp;nnFrom, GNeuralNetLayer *pNNToLayer, GBackPropNeuron &amp;bpFrom, double learningRate, double momentum)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::adjustWeightsSingleNeuron </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neuron.html">GNeuron</a> &amp;&#160;</td>
          <td class="paramname"><em>nnFrom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_neuron.html">GBackPropNeuron</a> &amp;&#160;</td>
          <td class="paramname"><em>bpFrom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>momentum</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Adjust the weights of a single neuron that follows a hidden layer. (Assumes the error of this neuron has already been computed). </p>

</div>
</div>
<a class="anchor" id="a71d0f533c7ae13baf0780a5ac8aa9e57"></a><!-- doxytag: member="GClasses::GBackProp::backpropagate" ref="a71d0f533c7ae13baf0780a5ac8aa9e57" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::backpropagate </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This method assumes that the error term is already set at every unit in the output layer. It uses back-propagation to compute the error term at every hidden unit. (It does not update any weights.) </p>

</div>
</div>
<a class="anchor" id="a7c0388ce51ff18b368fd00a32b6a19c5"></a><!-- doxytag: member="GClasses::GBackProp::backpropagateSingleOutput" ref="a7c0388ce51ff18b368fd00a32b6a19c5" args="(size_t outputNode)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::backpropagateSingleOutput </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>outputNode</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Backpropagates error from a single output node over all of the hidden layers. (Assumes the error term is already set on the specified output node.) </p>

</div>
</div>
<a class="anchor" id="a20f93ce50af7554ad32c3b4c703c2316"></a><!-- doxytag: member="GClasses::GBackProp::backPropFromSingleNode" ref="a20f93ce50af7554ad32c3b4c703c2316" args="(GNeuron &amp;nnFrom, GBackPropNeuron &amp;bpFrom, GNeuralNetLayer *pNNToLayer, GBackPropLayer *pBPToLayer)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::backPropFromSingleNode </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neuron.html">GNeuron</a> &amp;&#160;</td>
          <td class="paramname"><em>nnFrom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_neuron.html">GBackPropNeuron</a> &amp;&#160;</td>
          <td class="paramname"><em>bpFrom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&#160;</td>
          <td class="paramname"><em>pBPToLayer</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Backpropagates the error from a single output node to a hidden layer. </p>

</div>
</div>
<a class="anchor" id="a4648bfa8f89eb1d6abd757f7d36491bb"></a><!-- doxytag: member="GClasses::GBackProp::backPropLayer" ref="a4648bfa8f89eb1d6abd757f7d36491bb" args="(GNeuralNetLayer *pNNFromLayer, GNeuralNetLayer *pNNToLayer, GBackPropLayer *pBPFromLayer, GBackPropLayer *pBPToLayer, size_t fromBegin=0)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">static void GClasses::GBackProp::backPropLayer </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&#160;</td>
          <td class="paramname"><em>pBPFromLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&#160;</td>
          <td class="paramname"><em>pBPToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>fromBegin</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Backpropagates the error from the "from" layer to the "to" layer. (If the "to" layer has fewer units than the "from" layer, then it will begin propagating with the (fromBegin+1)th weight and stop when the "to" layer runs out of units. It would be an error if the number of units in the "from" layer is less than the number of units in the "to" layer plus fromBegin. </p>

</div>
</div>
<a class="anchor" id="a4ffe72bd0d4ae38ffa51afef9f7c5537"></a><!-- doxytag: member="GClasses::GBackProp::backPropLayer2" ref="a4ffe72bd0d4ae38ffa51afef9f7c5537" args="(GNeuralNetLayer *pNNFromLayer1, GNeuralNetLayer *pNNFromLayer2, GNeuralNetLayer *pNNToLayer, GBackPropLayer *pBPFromLayer1, GBackPropLayer *pBPFromLayer2, GBackPropLayer *pBPToLayer, size_t pass)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">static void GClasses::GBackProp::backPropLayer2 </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNFromLayer1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNFromLayer2</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> *&#160;</td>
          <td class="paramname"><em>pNNToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&#160;</td>
          <td class="paramname"><em>pBPFromLayer1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&#160;</td>
          <td class="paramname"><em>pBPFromLayer2</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a> *&#160;</td>
          <td class="paramname"><em>pBPToLayer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>pass</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This is another implementation of backPropLayer. This one is somewhat more flexible, but slightly less efficient. It supports backpropagating error from one or two layers. (pNNFromLayer2 should be NULL if you are backpropagating from just one layer.) It also supports temporal backpropagation by unfolding in time and then averaging the error across all of the unfolded instantiations. "pass" specifies how much of the error for this pass to accept. 1=all of it, 2=half of it, 3=one third, etc. </p>

</div>
</div>
<a class="anchor" id="a2e9f7e130763bd31e679e6570e66fcf3"></a><!-- doxytag: member="GClasses::GBackProp::descendGradient" ref="a2e9f7e130763bd31e679e6570e66fcf3" args="(const double *pFeatures, double learningRate, double momentum, bool useInputBias)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::descendGradient </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>momentum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>useInputBias</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This method assumes that the error term is already set for every network unit. It adjusts weights to descend the gradient of the error surface with respect to the weights. </p>

</div>
</div>
<a class="anchor" id="a325e169c24a186eaad819eae6e3be0d5"></a><!-- doxytag: member="GClasses::GBackProp::descendGradientSingleOutput" ref="a325e169c24a186eaad819eae6e3be0d5" args="(size_t outputNeuron, const double *pFeatures, double learningRate, double momentum, bool useInputBias)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GBackProp::descendGradientSingleOutput </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>outputNeuron</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>learningRate</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>momentum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>useInputBias</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This method assumes that the error term has been set for a single output network unit, and all units that feed into it transitively. It adjusts weights to descend the gradient of the error surface with respect to the weights. </p>

</div>
</div>
<a class="anchor" id="a84669ea9fd994062855d647248ad4a4e"></a><!-- doxytag: member="GClasses::GBackProp::layer" ref="a84669ea9fd994062855d647248ad4a4e" args="(size_t layer)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a>&amp; GClasses::GBackProp::layer </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>layer</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns a layer (not a layer of the neural network, but a corresponding layer of values used for back-prop) </p>

</div>
</div>
<hr/><h2>Friends And Related Function Documentation</h2>
<a class="anchor" id="a9311a6752671c2a8e3cf8d8aacff043e"></a><!-- doxytag: member="GClasses::GBackProp::GNeuralNet" ref="a9311a6752671c2a8e3cf8d8aacff043e" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">friend class <a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a><code> [friend]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<hr/><h2>Member Data Documentation</h2>
<a class="anchor" id="a5e01ba2da853068b910c71072459ec0e"></a><!-- doxytag: member="GClasses::GBackProp::m_layers" ref="a5e01ba2da853068b910c71072459ec0e" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;<a class="el" href="class_g_classes_1_1_g_back_prop_layer.html">GBackPropLayer</a>&gt; <a class="el" href="class_g_classes_1_1_g_back_prop.html#a5e01ba2da853068b910c71072459ec0e">GClasses::GBackProp::m_layers</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="af1484d812296d23a0b332942f5bfff73"></a><!-- doxytag: member="GClasses::GBackProp::m_pNN" ref="af1484d812296d23a0b332942f5bfff73" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a>* <a class="el" href="class_g_classes_1_1_g_back_prop.html#af1484d812296d23a0b332942f5bfff73">GClasses::GBackProp::m_pNN</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
</div>
</div>
  <div id="nav-path" class="navpath">
    <ul>
      <li class="navelem"><a class="el" href="namespace_g_classes.html">GClasses</a>      </li>
      <li class="navelem"><a class="el" href="class_g_classes_1_1_g_back_prop.html">GBackProp</a>      </li>
      <li class="footer">Generated on Mon Dec 5 2011 14:19:02 for GClasses by&#160;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.7.3 </li>
    </ul>
  </div>

</body>
</html>
