<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>GClasses: GClasses::GQLearner Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript">
$(document).ready(initResizable);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.7.3 -->
<div id="top">
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">GClasses</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
</div>
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
  initNavTree('class_g_classes_1_1_g_q_learner.html','');
</script>
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a>  </div>
  <div class="headertitle">
<h1>GClasses::GQLearner Class Reference</h1>  </div>
</div>
<div class="contents">
<!-- doxytag: class="GClasses::GQLearner" --><!-- doxytag: inherits="GClasses::GPolicyLearner" -->
<p>The base class of a Q-Learner. To use this class, there are four abstract methods you'll need to implement. See also the comment for <a class="el" href="class_g_classes_1_1_g_policy_learner.html" title="This is the base class for algorithms that learn a policy.">GPolicyLearner</a>.  
<a href="#_details">More...</a></p>

<p><code>#include &lt;GReinforcement.h&gt;</code></p>
<div class="dynheader">
Inheritance diagram for GClasses::GQLearner:</div>
<div class="dyncontent">
 <div class="center">
  <img src="class_g_classes_1_1_g_q_learner.png" usemap="#GClasses::GQLearner_map" alt=""/>
  <map id="GClasses::GQLearner_map" name="GClasses::GQLearner_map">
<area href="class_g_classes_1_1_g_policy_learner.html" alt="GClasses::GPolicyLearner" shape="rect" coords="0,0,233,24"/>
<area href="class_g_classes_1_1_g_incremental_learner_q_agent.html" alt="GClasses::GIncrementalLearnerQAgent" shape="rect" coords="0,112,233,136"/>
</map>
 </div></div>

<p><a href="class_g_classes_1_1_g_q_learner-members.html">List of all members.</a></p>
<table class="memberdecls">
<tr><td colspan="2"><h2><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a4a3fbd07012a939e8f0a3e19b1aca741">GQLearner</a> (<a class="el" href="class_g_classes_1_1smart__ptr.html">sp_relation</a> &amp;pRelation, int actionDims, double *pInitialState, <a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a> *pRand, <a class="el" href="class_g_classes_1_1_g_agent_action_iterator.html">GAgentActionIterator</a> *pActionIterator)</td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#af04d9ad3015d406fae0a1a339c43bd87">~GQLearner</a> ()</td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a818802e429a17efe90310623026f1c19">setLearningRate</a> (double d)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the learning rate (often called "alpha"). If state is deterministic and actions have deterministic consequences, then this should be 1. If there is any non-determinism, there are three common approaches for picking the learning rate: 1- use a fairly small value (perhaps 0.1), 2- decay it over time (by calling this method before every iteration), 3- remember how many times 'n' each state has already been visited, and set the learning rate to 1/(n+1) before each iteration. The third technique is the best, but is awkward with continuous state spaces.  <a href="#a818802e429a17efe90310623026f1c19"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a019fae7b0500332ee9947cc0e3cb1362">setDiscountFactor</a> (double d)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the factor for discounting future rewards (often called "gamma").  <a href="#a019fae7b0500332ee9947cc0e3cb1362"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a85b97fc5300546db37e92098ddd069f9">getQValue</a> (const double *pState, const double *pAction)=0</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">You must implement some kind of structure to store q-values. This method should return the current q-value for the specified state and action.  <a href="#a85b97fc5300546db37e92098ddd069f9"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a3d03ef30220ecb29362a77515a51fb7e">setQValue</a> (const double *pState, const double *pAction, double qValue)=0</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This is the complement to GetQValue.  <a href="#a3d03ef30220ecb29362a77515a51fb7e"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a2acb4fc62c4c5863e940e3d85ce1e089">refinePolicyAndChooseNextAction</a> (const double *pSenses, double *pOutActions)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See <a class="el" href="class_g_classes_1_1_g_policy_learner.html#a4aefa73ddefd3144f5a64ded2c774315" title="This method tells the agent to learn from the current senses, and select a new action vector...">GPolicyLearner::refinePolicyAndChooseNextAction</a>.  <a href="#a2acb4fc62c4c5863e940e3d85ce1e089"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a3dc50ef1679e8ea8c7e9057ee0cde2b4">setActionCap</a> (int n)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This specifies a cap on how many actions to sample. (If actions are continuous, you obviously don't want to try them all.)  <a href="#a3dc50ef1679e8ea8c7e9057ee0cde2b4"></a><br/></td></tr>
<tr><td colspan="2"><h2><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a40f05909e96c2dd4990a6a242dcc6453">chooseAction</a> (const double *pSenses, double *pOutActions)=0</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This method picks the action during training. This method is called by refinePolicyAndChooseNextAction. (If it makes things easier, the agent may actually perform the action here, but it's a better practise to wait until refinePolicyAndChooseNextAction returns, because that keeps the "thinking" and "acting" stages separated from each other.) One way to pick the next action is to call GetQValue for all possible actions in the current state, and pick the one with the highest Q-value. But if you always pick the best action, you'll never discover things you don't already know about, so you need to find some balance between exploration and exploitation. One way to do this is to usually pick the best action, but sometimes pick a random action.  <a href="#a40f05909e96c2dd4990a6a242dcc6453"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#ae4ebe241f6bd5055a73e93c444ad54e0">rewardFromLastAction</a> ()=0</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">A reward is obtained when the agent performs a particular action in a particular state. (A penalty is a negative reward. A reward of zero is no reward.) This method returns the reward that was obtained when the last action was performed. If you return UNKNOWN_REAL_VALUE, then the q-table will not be updated for that action.  <a href="#ae4ebe241f6bd5055a73e93c444ad54e0"></a><br/></td></tr>
<tr><td colspan="2"><h2><a name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#af55d40710e61201cf7cc286f973498c5">m_pRand</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_agent_action_iterator.html">GAgentActionIterator</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#afd7fa567aab42860ec1c756557027f7d">m_pActionIterator</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a4e93f86d57c064d4d451165cef670024">m_learningRate</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a168f9083ea8f02667b920bf299d9bfe5">m_discountFactor</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#aec9a8a3b5cd0f70855777aa129d5545c">m_pSenses</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a8fa363f886412d77f698be9a3ad4e5ba">m_pAction</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_q_learner.html#a10bac91947799c42bfde50bdf9e8efd6">m_actionCap</a></td></tr>
</table>
<hr/><a name="_details"></a><h2>Detailed Description</h2>
<div class="textblock"><p>The base class of a Q-Learner. To use this class, there are four abstract methods you'll need to implement. See also the comment for <a class="el" href="class_g_classes_1_1_g_policy_learner.html" title="This is the base class for algorithms that learn a policy.">GPolicyLearner</a>. </p>
</div><hr/><h2>Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="a4a3fbd07012a939e8f0a3e19b1aca741"></a><!-- doxytag: member="GClasses::GQLearner::GQLearner" ref="a4a3fbd07012a939e8f0a3e19b1aca741" args="(sp_relation &amp;pRelation, int actionDims, double *pInitialState, GRand *pRand, GAgentActionIterator *pActionIterator)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">GClasses::GQLearner::GQLearner </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1smart__ptr.html">sp_relation</a> &amp;&#160;</td>
          <td class="paramname"><em>pRelation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>actionDims</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pInitialState</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a> *&#160;</td>
          <td class="paramname"><em>pRand</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_agent_action_iterator.html">GAgentActionIterator</a> *&#160;</td>
          <td class="paramname"><em>pActionIterator</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="af04d9ad3015d406fae0a1a339c43bd87"></a><!-- doxytag: member="GClasses::GQLearner::~GQLearner" ref="af04d9ad3015d406fae0a1a339c43bd87" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual GClasses::GQLearner::~GQLearner </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<hr/><h2>Member Function Documentation</h2>
<a class="anchor" id="a40f05909e96c2dd4990a6a242dcc6453"></a><!-- doxytag: member="GClasses::GQLearner::chooseAction" ref="a40f05909e96c2dd4990a6a242dcc6453" args="(const double *pSenses, double *pOutActions)=0" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GQLearner::chooseAction </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pSenses</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pOutActions</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [protected, pure virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This method picks the action during training. This method is called by refinePolicyAndChooseNextAction. (If it makes things easier, the agent may actually perform the action here, but it's a better practise to wait until refinePolicyAndChooseNextAction returns, because that keeps the "thinking" and "acting" stages separated from each other.) One way to pick the next action is to call GetQValue for all possible actions in the current state, and pick the one with the highest Q-value. But if you always pick the best action, you'll never discover things you don't already know about, so you need to find some balance between exploration and exploitation. One way to do this is to usually pick the best action, but sometimes pick a random action. </p>

<p>Implemented in <a class="el" href="class_g_classes_1_1_g_incremental_learner_q_agent.html#a21ae8e8bae94fbc24d797f61f5cbc6c5">GClasses::GIncrementalLearnerQAgent</a>.</p>

</div>
</div>
<a class="anchor" id="a85b97fc5300546db37e92098ddd069f9"></a><!-- doxytag: member="GClasses::GQLearner::getQValue" ref="a85b97fc5300546db37e92098ddd069f9" args="(const double *pState, const double *pAction)=0" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual double GClasses::GQLearner::getQValue </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pState</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pAction</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [pure virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>You must implement some kind of structure to store q-values. This method should return the current q-value for the specified state and action. </p>

<p>Implemented in <a class="el" href="class_g_classes_1_1_g_incremental_learner_q_agent.html#a408da3ccfb79be4da4ce5b94e135417a">GClasses::GIncrementalLearnerQAgent</a>.</p>

</div>
</div>
<a class="anchor" id="a2acb4fc62c4c5863e940e3d85ce1e089"></a><!-- doxytag: member="GClasses::GQLearner::refinePolicyAndChooseNextAction" ref="a2acb4fc62c4c5863e940e3d85ce1e089" args="(const double *pSenses, double *pOutActions)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GQLearner::refinePolicyAndChooseNextAction </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pSenses</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pOutActions</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See <a class="el" href="class_g_classes_1_1_g_policy_learner.html#a4aefa73ddefd3144f5a64ded2c774315" title="This method tells the agent to learn from the current senses, and select a new action vector...">GPolicyLearner::refinePolicyAndChooseNextAction</a>. </p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_policy_learner.html#a4aefa73ddefd3144f5a64ded2c774315">GClasses::GPolicyLearner</a>.</p>

</div>
</div>
<a class="anchor" id="ae4ebe241f6bd5055a73e93c444ad54e0"></a><!-- doxytag: member="GClasses::GQLearner::rewardFromLastAction" ref="ae4ebe241f6bd5055a73e93c444ad54e0" args="()=0" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual double GClasses::GQLearner::rewardFromLastAction </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [protected, pure virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>A reward is obtained when the agent performs a particular action in a particular state. (A penalty is a negative reward. A reward of zero is no reward.) This method returns the reward that was obtained when the last action was performed. If you return UNKNOWN_REAL_VALUE, then the q-table will not be updated for that action. </p>

</div>
</div>
<a class="anchor" id="a3dc50ef1679e8ea8c7e9057ee0cde2b4"></a><!-- doxytag: member="GClasses::GQLearner::setActionCap" ref="a3dc50ef1679e8ea8c7e9057ee0cde2b4" args="(int n)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GQLearner::setActionCap </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>n</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This specifies a cap on how many actions to sample. (If actions are continuous, you obviously don't want to try them all.) </p>

</div>
</div>
<a class="anchor" id="a019fae7b0500332ee9947cc0e3cb1362"></a><!-- doxytag: member="GClasses::GQLearner::setDiscountFactor" ref="a019fae7b0500332ee9947cc0e3cb1362" args="(double d)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GQLearner::setDiscountFactor </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>d</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Sets the factor for discounting future rewards (often called "gamma"). </p>

</div>
</div>
<a class="anchor" id="a818802e429a17efe90310623026f1c19"></a><!-- doxytag: member="GClasses::GQLearner::setLearningRate" ref="a818802e429a17efe90310623026f1c19" args="(double d)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GQLearner::setLearningRate </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>d</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Sets the learning rate (often called "alpha"). If state is deterministic and actions have deterministic consequences, then this should be 1. If there is any non-determinism, there are three common approaches for picking the learning rate: 1- use a fairly small value (perhaps 0.1), 2- decay it over time (by calling this method before every iteration), 3- remember how many times 'n' each state has already been visited, and set the learning rate to 1/(n+1) before each iteration. The third technique is the best, but is awkward with continuous state spaces. </p>

</div>
</div>
<a class="anchor" id="a3d03ef30220ecb29362a77515a51fb7e"></a><!-- doxytag: member="GClasses::GQLearner::setQValue" ref="a3d03ef30220ecb29362a77515a51fb7e" args="(const double *pState, const double *pAction, double qValue)=0" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GQLearner::setQValue </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pState</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pAction</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>qValue</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [pure virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This is the complement to GetQValue. </p>

<p>Implemented in <a class="el" href="class_g_classes_1_1_g_incremental_learner_q_agent.html#ae02583c57303a86e45af9f631bbed665">GClasses::GIncrementalLearnerQAgent</a>.</p>

</div>
</div>
<hr/><h2>Member Data Documentation</h2>
<a class="anchor" id="a10bac91947799c42bfde50bdf9e8efd6"></a><!-- doxytag: member="GClasses::GQLearner::m_actionCap" ref="a10bac91947799c42bfde50bdf9e8efd6" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int <a class="el" href="class_g_classes_1_1_g_q_learner.html#a10bac91947799c42bfde50bdf9e8efd6">GClasses::GQLearner::m_actionCap</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a168f9083ea8f02667b920bf299d9bfe5"></a><!-- doxytag: member="GClasses::GQLearner::m_discountFactor" ref="a168f9083ea8f02667b920bf299d9bfe5" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="class_g_classes_1_1_g_q_learner.html#a168f9083ea8f02667b920bf299d9bfe5">GClasses::GQLearner::m_discountFactor</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a4e93f86d57c064d4d451165cef670024"></a><!-- doxytag: member="GClasses::GQLearner::m_learningRate" ref="a4e93f86d57c064d4d451165cef670024" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="class_g_classes_1_1_g_q_learner.html#a4e93f86d57c064d4d451165cef670024">GClasses::GQLearner::m_learningRate</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a8fa363f886412d77f698be9a3ad4e5ba"></a><!-- doxytag: member="GClasses::GQLearner::m_pAction" ref="a8fa363f886412d77f698be9a3ad4e5ba" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double* <a class="el" href="class_g_classes_1_1_g_q_learner.html#a8fa363f886412d77f698be9a3ad4e5ba">GClasses::GQLearner::m_pAction</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="afd7fa567aab42860ec1c756557027f7d"></a><!-- doxytag: member="GClasses::GQLearner::m_pActionIterator" ref="afd7fa567aab42860ec1c756557027f7d" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_agent_action_iterator.html">GAgentActionIterator</a>* <a class="el" href="class_g_classes_1_1_g_q_learner.html#afd7fa567aab42860ec1c756557027f7d">GClasses::GQLearner::m_pActionIterator</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="af55d40710e61201cf7cc286f973498c5"></a><!-- doxytag: member="GClasses::GQLearner::m_pRand" ref="af55d40710e61201cf7cc286f973498c5" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a>* <a class="el" href="class_g_classes_1_1_g_q_learner.html#af55d40710e61201cf7cc286f973498c5">GClasses::GQLearner::m_pRand</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="aec9a8a3b5cd0f70855777aa129d5545c"></a><!-- doxytag: member="GClasses::GQLearner::m_pSenses" ref="aec9a8a3b5cd0f70855777aa129d5545c" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double* <a class="el" href="class_g_classes_1_1_g_q_learner.html#aec9a8a3b5cd0f70855777aa129d5545c">GClasses::GQLearner::m_pSenses</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
</div>
</div>
  <div id="nav-path" class="navpath">
    <ul>
      <li class="navelem"><a class="el" href="namespace_g_classes.html">GClasses</a>      </li>
      <li class="navelem"><a class="el" href="class_g_classes_1_1_g_q_learner.html">GQLearner</a>      </li>
      <li class="footer">Generated on Mon Dec 5 2011 14:19:03 for GClasses by&#160;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.7.3 </li>
    </ul>
  </div>

</body>
</html>
