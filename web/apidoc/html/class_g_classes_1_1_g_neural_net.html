<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>GClasses: GClasses::GNeuralNet Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript">
$(document).ready(initResizable);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.7.3 -->
<div id="top">
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">GClasses</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
</div>
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
  initNavTree('class_g_classes_1_1_g_neural_net.html','');
</script>
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-types">Public Types</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a> &#124;
<a href="#friends">Friends</a>  </div>
  <div class="headertitle">
<h1>GClasses::GNeuralNet Class Reference</h1>  </div>
</div>
<div class="contents">
<!-- doxytag: class="GClasses::GNeuralNet" --><!-- doxytag: inherits="GClasses::GIncrementalLearner" -->
<p>An artificial neural network.  
<a href="#_details">More...</a></p>

<p><code>#include &lt;GNeuralNet.h&gt;</code></p>
<div class="dynheader">
Inheritance diagram for GClasses::GNeuralNet:</div>
<div class="dyncontent">
 <div class="center">
  <img src="class_g_classes_1_1_g_neural_net.png" usemap="#GClasses::GNeuralNet_map" alt=""/>
  <map id="GClasses::GNeuralNet_map" name="GClasses::GNeuralNet_map">
<area href="class_g_classes_1_1_g_incremental_learner.html" alt="GClasses::GIncrementalLearner" shape="rect" coords="0,112,190,136"/>
<area href="class_g_classes_1_1_g_supervised_learner.html" alt="GClasses::GSupervisedLearner" shape="rect" coords="0,56,190,80"/>
<area href="class_g_classes_1_1_g_transducer.html" alt="GClasses::GTransducer" shape="rect" coords="0,0,190,24"/>
</map>
 </div></div>

<p><a href="class_g_classes_1_1_g_neural_net-members.html">List of all members.</a></p>
<table class="memberdecls">
<tr><td colspan="2"><h2><a name="pub-types"></a>
Public Types</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a> { <a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84a6b9faea53545aa616825e2885f6816eb">squared_error</a>, 
<a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84a9c238cea3a50aa79304e64ac443e991b">cross_entropy</a>, 
<a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84a46c15a6073e266659036be96f12e2a69">sign</a>, 
<a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84ac01a705fd048c4ecbc78b52f6fde378a">physical</a>
 }</td></tr>
<tr><td colspan="2"><h2><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a899899ef6ba0eb9526567322bee7c413">GNeuralNet</a> (<a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a> &amp;rand)</td></tr>
<tr><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a6e8cbe8626913a36df44f5a843c1ca36">GNeuralNet</a> (<a class="el" href="class_g_classes_1_1_g_dom_node.html">GDomNode</a> *pNode, <a class="el" href="class_g_classes_1_1_g_learner_loader.html">GLearnerLoader</a> &amp;ll)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Load from a text-format.  <a href="#a6e8cbe8626913a36df44f5a843c1ca36"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a090fea822a920c75c99389aae9b2ebc7">~GNeuralNet</a> ()</td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual <a class="el" href="class_g_classes_1_1_g_dom_node.html">GDomNode</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a1d36eb292d21354a5e14b82f3359fb4e">serialize</a> (<a class="el" href="class_g_classes_1_1_g_dom.html">GDom</a> *pDoc)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Saves the model to a text file.  <a href="#a1d36eb292d21354a5e14b82f3359fb4e"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3abb685af4764fa0f0c34492799f91b1">setActivationFunction</a> (<a class="el" href="class_g_classes_1_1_g_activation_function.html">GActivationFunction</a> *pSF, bool hold)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the activation function to use with all subsequently added layers. (Note that the activation function for the output layer is set when train or beginIncrementalLearning is called, so if you only wish to set the squshing function for the output layer, call this method after all hidden layers have been added, but before you call train.) If hold is true, then the neural network will hold on to this instance of the activation function and delete it when the neural network is deleted.  <a href="#a3abb685af4764fa0f0c34492799f91b1"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a269ad30a3d161a3069cf16ffdd6746d3">addLayer</a> (size_t nNodes)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a hidden layer to the network. (The first hidden layer that you add will be adjacent to the input features. The last hidden layer that you add will be adjacent to the output layer.)  <a href="#a269ad30a3d161a3069cf16ffdd6746d3"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a449d358ad9956d5fc61db21efe1a3942">layerCount</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the number of layers in this neural network. (Every network has at least one output layer, plus all of the hidden layers that you add by calling addLayer. The input vector does not count as a layer, even though it may be common to visualize it as a layer.)  <a href="#a449d358ad9956d5fc61db21efe1a3942"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ad70a9ed88aca050141139babb790f516">layer</a> (size_t n)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns a reference to the specified layer.  <a href="#ad70a9ed88aca050141139babb790f516"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#aecbaee8213cc4cdf64aff5842ac7f255">addNode</a> (size_t layer)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a new node at the end of the specified layer. (The new node is initialized with small weights, so this operation should initially have little impact on predictions.)  <a href="#aecbaee8213cc4cdf64aff5842ac7f255"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ad7aba05b75ee2d29d67bab3db14f5af7">dropNode</a> (size_t layer, size_t node)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Removes the specified node from the specified layer. (An exception will be thrown the layer only has one node.)  <a href="#ad7aba05b75ee2d29d67bab3db14f5af7"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_back_prop.html">GBackProp</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#afde11cca4167200c72b6567ef03b50fa">backProp</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the backprop object associated with this neural net (if there is one)  <a href="#afde11cca4167200c72b6567ef03b50fa"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a0f3e41a0e0e9e363e2398de2ba231f33">setValidationPortion</a> (double d)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the portion of the data that will be used for validation. If the value is 0, then all of the data is used for both training and validation.  <a href="#a0f3e41a0e0e9e363e2398de2ba231f33"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a62993759833ca851fd776a5ac0baa2b9">countWeights</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Counts the number of weights in the network. (This value is not cached, so you should cache it rather than frequently call this method.)  <a href="#a62993759833ca851fd776a5ac0baa2b9"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a85534d9115c3ab715d3043715e6c2145">perturbAllWeights</a> (double deviation)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Perturbs all weights in the network by a random normal offset with the specified deviation.  <a href="#a85534d9115c3ab715d3043715e6c2145"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a9ce8cc5daefb05c2fc60b2c2f1257952">clipWeights</a> (double max)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Clips all non-bias weights to fall within the range [-max, max].  <a href="#a9ce8cc5daefb05c2fc60b2c2f1257952"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#abc535e04fa4d64fa2c641d3c1f5a058f">decayWeights</a> (double lambda, double gamma=1.0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Multiplies all non-bias weights by (1.0 - (learning_rate * lambda)), starting with the output layer, and ending with the first hidden layer. Typical values for lambda are small (like 0.001.) After each layer, the value of lambda is multiplied by gamma. (If gamma is greater than 1.0, then weights in hidden layers will decay faster, and if gamma is less than 1.0, then weights in hidden layers will decay slower.) It may be significant to note that if a regularizing penalty is added to the error of lambda times the sum-squared values of non-bias weights, then on-line weight updating works out to the same as decaying the weights after each application of back-prop.  <a href="#abc535e04fa4d64fa2c641d3c1f5a058f"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a96d7b1f6807b1ccd1110c7e6650015b0">decayWeightsSingleOutput</a> (size_t output, double lambda)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Just like decayWeights, except it only decays the weights in one of the output units.  <a href="#a96d7b1f6807b1ccd1110c7e6650015b0"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a40d6982c5565bdb16711ab0361471614">learningRate</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the current learning rate.  <a href="#a40d6982c5565bdb16711ab0361471614"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#abfdf6d4186899d5f508d32a336903594">setLearningRate</a> (double d)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the learning rate.  <a href="#abfdf6d4186899d5f508d32a336903594"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#aaae62462d9be256a2a260c17eef4afbc">momentum</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the current momentum value.  <a href="#aaae62462d9be256a2a260c17eef4afbc"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3294a63458679248ed777c433ae3c66d">setMomentum</a> (double d)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Momentum has the effect of speeding convergence and helping the gradient descent algorithm move past some local minimums.  <a href="#a3294a63458679248ed777c433ae3c66d"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a1fef0a2e6706f31d1d47f6fdefa328f1">improvementThresh</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the threshold ratio for improvement.  <a href="#a1fef0a2e6706f31d1d47f6fdefa328f1"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ac12aa4d7571dcb380ee03da2a1634854">setImprovementThresh</a> (double d)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies the threshold ratio for improvement that must be made since the last validation check for training to continue. (For example, if the mean squared error at the previous validation check was 50, and the mean squared error at the current validation check is 49, then training will stop if d is &gt; 0.02.)  <a href="#ac12aa4d7571dcb380ee03da2a1634854"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a30752f8ebc861f868d8d539a8a41399a">windowSize</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the number of epochs to perform before the validation data is evaluated to see if training should stop.  <a href="#a30752f8ebc861f868d8d539a8a41399a"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#add517c7b14f72ce910e10f9e3eb9ae38">setWindowSize</a> (size_t n)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the number of epochs that will be performed before each time the network is tested again with the validation set to determine if we have a better best-set of weights, and whether or not it's achieved the termination condition yet. (An epochs is defined as a single pass through all rows in the training set.)  <a href="#add517c7b14f72ce910e10f9e3eb9ae38"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3942e81858018f7977e97a99fd222254">setBackPropTargetFunction</a> (<a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a> eTF)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Specify the target function to use for back-propagation. The default is squared_error. cross_entropy tends to be faster, and is well-suited for classification tasks.  <a href="#a3942e81858018f7977e97a99fd222254"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ad5187a12369a854003fc455a36a6284e">backPropTargetFunction</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the enumeration of the target function used for backpropagation.  <a href="#ad5187a12369a854003fc455a36a6284e"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3c726ba4f3a5924dd0cf4130b27529d9">trainSparse</a> (<a class="el" href="class_g_classes_1_1_g_sparse_matrix.html">GSparseMatrix</a> &amp;features, <a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;labels)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_incremental_learner.html#a25ddbd1ac0eb49543f8a752585082c50" title="Train using a sparse feature matrix. (A Typical implementation of this method will first call beginIn...">GIncrementalLearner::trainSparse</a> Assumes all attributes are continuous.  <a href="#a3c726ba4f3a5924dd0cf4130b27529d9"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a034d339a9946207f5cf20478c96ffdad">clear</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#aa4df3b300514838026aa3eda33315fb4" title="Discards all training for the purpose of freeing memory. If you call this method, you must train befo...">GSupervisedLearner::clear</a>.  <a href="#a034d339a9946207f5cf20478c96ffdad"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a6966d563836d90cb814ec1d69e0e2b06">trainWithValidation</a> (<a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;trainFeatures, <a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;trainLabels, <a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;validateFeatures, <a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;validateLabels)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Train the network until the termination condition is met. Returns the number of epochs required to train it.  <a href="#a6966d563836d90cb814ec1d69e0e2b06"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#aed2999a007c3fcf02e484e9d87c627d5">releaseTrainingJunk</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Some extra junk is allocated when training to make it efficient. This method is called when training is done to get rid of that extra junk.  <a href="#aed2999a007c3fcf02e484e9d87c627d5"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a4041182ca3c44c4c9cee771305518cee">internalTraininGMatrix</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the internal training data set.  <a href="#a4041182ca3c44c4c9cee771305518cee"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a376fb711d15ec0ed160a64c3e4ccc249">internalValidationData</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Gets the internal validation data set.  <a href="#a376fb711d15ec0ed160a64c3e4ccc249"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ac7b2fc220e96389fc0cb4d3694d40a02">setWeights</a> (const double *pWeights)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets all the weights from an array of doubles. The number of doubles in the array can be determined by calling <a class="el" href="class_g_classes_1_1_g_neural_net.html#a62993759833ca851fd776a5ac0baa2b9" title="Counts the number of weights in the network. (This value is not cached, so you should cache it rather...">countWeights()</a>.  <a href="#ac7b2fc220e96389fc0cb4d3694d40a02"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#af2de389bfa8fc82fc4c4131b277b1f3b">copyWeights</a> (<a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *pOther)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Copy the weights from pOther. It is assumed (but not checked) that pOther has the same network structure as this neural network.  <a href="#af2de389bfa8fc82fc4c4131b277b1f3b"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a93626ba454ef61f90b87df71a267b43e">copyStructure</a> (<a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *pOther)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Copies the layers, nodes, and settings from pOther (but not the weights). beginIncrementalLearning must have been called on pOther so that it has a complete structure.  <a href="#a93626ba454ef61f90b87df71a267b43e"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a48f738fa5bc10067b2901cc12624e3bb">weights</a> (double *pOutWeights)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Serializes the network weights into an array of doubles. The number of doubles in the array can be determined by calling <a class="el" href="class_g_classes_1_1_g_neural_net.html#a62993759833ca851fd776a5ac0baa2b9" title="Counts the number of weights in the network. (This value is not cached, so you should cache it rather...">countWeights()</a>.  <a href="#a48f738fa5bc10067b2901cc12624e3bb"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a0f5e4c4d72d08a8219b95b7e059ae7cc">forwardProp</a> (const double *pInputs)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Evaluates a feature vector. (The results will be in the nodes of the output layer.)  <a href="#a0f5e4c4d72d08a8219b95b7e059ae7cc"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a625914c24e4a7ca1bb35d80317e9f409">forwardPropSingleOutput</a> (const double *pInputs, size_t output)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This is the same as forwardProp, except it only propagates to a single output node. It returns the value that this node outputs.  <a href="#a625914c24e4a7ca1bb35d80317e9f409"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ae87bc9372fb5705cf3bde1481faef090">copyPrediction</a> (double *pOut)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This method assumes forwardProp has been called. It copies the predicted vector into pOut.  <a href="#ae87bc9372fb5705cf3bde1481faef090"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a4fc5b7d1e52af80cd3676a759e1bccc2">sumSquaredPredictionError</a> (const double *pTarget)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This method assumes forwardProp has been called. It computes the sum squared prediction error with the specified target vector.  <a href="#a4fc5b7d1e52af80cd3676a759e1bccc2"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a2d02f321a6232f3b1640504c235655c9">setErrorOnOutputLayer</a> (const double *pTarget, <a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a> eTargetFunction=squared_error)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This method assumes that forwardProp has already been called. (Note that the predict method calls forwardProp). It computes the error values at each node in the output layer. After calling this method, it is typical to call <a class="el" href="class_g_classes_1_1_g_neural_net.html#afde11cca4167200c72b6567ef03b50fa" title="Returns the backprop object associated with this neural net (if there is one)">backProp()</a>-&gt;backpropagate(), to compute the error on the hidden nodes, and then to call <a class="el" href="class_g_classes_1_1_g_neural_net.html#afde11cca4167200c72b6567ef03b50fa" title="Returns the backprop object associated with this neural net (if there is one)">backProp()</a>-&gt;descendGradient to update the weights. pTarget contains the target values for the ouptut nodes.  <a href="#a2d02f321a6232f3b1640504c235655c9"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ac19924e551a016c0916ae4ba561ea9ce">setErrorSingleOutput</a> (double target, size_t output, <a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a> eTargetFunction=squared_error)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This is teh same as setErrorOnOutputLayer, except that it only sets the error on a single output node.  <a href="#ac19924e551a016c0916ae4ba561ea9ce"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a253b2d399a42a25753b361ff1e758053">autoTune</a> (<a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;features, <a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;labels)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Uses cross-validation to find a set of parameters that works well with the provided data. That is, this method will add a good number of hidden layers, pick a good momentum value, etc.  <a href="#a253b2d399a42a25753b361ff1e758053"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a201f1bb4bca2d429938dec193a2af973">setUseInputBias</a> (bool b)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Specify whether to use an input bias. (The default is false.) This feature is used with generative-backpropagation, which adjusts inputs to create latent features.  <a href="#a201f1bb4bca2d429938dec193a2af973"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ad307984e3c04ecce690d3675a19cb735">useInputBias</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns whether this neural network utilizes an input bias.  <a href="#ad307984e3c04ecce690d3675a19cb735"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#aeb58b1ee21b56a3f38d532bce284e1b9">hasTrainingBegun</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns true iff train or beginIncrementalTraining has been called.  <a href="#aeb58b1ee21b56a3f38d532bce284e1b9"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a93ccc4e04cc4d70f6451f4eb5171b9f3">swapNodes</a> (size_t layer, size_t a, size_t b)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Swaps two nodes in the specified layer. If layer specifies one of the hidden layers, then this will have no net effect on the output of the network. (Assumes this model is already trained.)  <a href="#a93ccc4e04cc4d70f6451f4eb5171b9f3"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#adeceb444d141cfa17e2e245c1f6f3419">align</a> (<a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> &amp;that)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Swaps nodes in hidden layers of this neural network to align with those in that neural network, as determined using bipartite matching. (This might be done, for example, before averaging weights together.)  <a href="#adeceb444d141cfa17e2e245c1f6f3419"></a><br/></td></tr>
<tr><td colspan="2"><h2><a name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">static void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ab0b1fe168491730483217ccccf6cf374">test</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs unit tests for this class. Throws an exception if there is a failure.  <a href="#ab0b1fe168491730483217ccccf6cf374"></a><br/></td></tr>
<tr><td colspan="2"><h2><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a7f38fd2cac389cb0c96cd215607c1a04">validationSquaredError</a> (<a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;features, <a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;labels)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Measures the sum squared error against the specified dataset.  <a href="#a7f38fd2cac389cb0c96cd215607c1a04"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a903d83e71d80899a49c816769f54d360">trainInner</a> (<a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;features, <a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;labels)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#aa613836bbc260ba1665c7bfa3f5c5a6b" title="This is the implementation of the model&#39;s training algorithm. (This method is called by train)...">GSupervisedLearner::trainInner</a>.  <a href="#a903d83e71d80899a49c816769f54d360"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#aa201d1358f827511fc31389f4e8cedcc">predictInner</a> (const double *pIn, double *pOut)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#a4fec9ccd13a7cf09c87762f3073cba79" title="This is the implementation of the model&#39;s prediction algorithm. (This method is called by predict...">GSupervisedLearner::predictInner</a>.  <a href="#aa201d1358f827511fc31389f4e8cedcc"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a98495c7bfb4fa4cb68d36dafd2428fac">predictDistributionInner</a> (const double *pIn, <a class="el" href="class_g_classes_1_1_g_prediction.html">GPrediction</a> *pOut)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#a78853095b1557cdd6ceea9814f8a0a7a" title="This is the implementation of the model&#39;s prediction algorithm. (This method is called by predict...">GSupervisedLearner::predictDistributionInner</a>.  <a href="#a98495c7bfb4fa4cb68d36dafd2428fac"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a9507ad0abba2a4baafecc64f56422301">canImplicitlyHandleNominalFeatures</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#a07c1bac223ab1fc1eb369a10313688de" title="Returns true iff this algorithm can implicitly handle nominal features. If it cannot, then the GNominalToCat transform will be used to convert nominal features to continuous values before passing them to it.">GTransducer::canImplicitlyHandleNominalFeatures</a>.  <a href="#a9507ad0abba2a4baafecc64f56422301"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a18ca749935c5a40ce3a9e9ae138196ff">supportedFeatureRange</a> (double *pOutMin, double *pOutMax)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#ac7b2855e39b2897801641b42870dee20" title="Returns true if this algorithm supports any feature value, or if it does not implicitly handle contin...">GTransducer::supportedFeatureRange</a>.  <a href="#a18ca749935c5a40ce3a9e9ae138196ff"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ae82282648edc1a9a16f87e7b1ecadaa5">canImplicitlyHandleMissingFeatures</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#ad4557a177f7d0c92f26d40c102418e90" title="Returns true iff this algorithm supports missing feature values. If it cannot, then an imputation fil...">GTransducer::canImplicitlyHandleMissingFeatures</a>.  <a href="#ae82282648edc1a9a16f87e7b1ecadaa5"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a6528bdaa8bfdfc8174237791f3a54238">canImplicitlyHandleNominalLabels</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#a5b07f5b4775bb69cb982d64f0b75c430" title="Returns true iff this algorithm can implicitly handle nominal labels (a.k.a. classification). If it cannot, then the GNominalToCat transform will be used during training to convert nominal labels to continuous values, and to convert categorical predictions back to nominal labels.">GTransducer::canImplicitlyHandleNominalLabels</a>.  <a href="#a6528bdaa8bfdfc8174237791f3a54238"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a5d699a8170b1c4bea6d2d1bbcc6d5395">supportedLabelRange</a> (double *pOutMin, double *pOutMax)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#ac7b2855e39b2897801641b42870dee20" title="Returns true if this algorithm supports any feature value, or if it does not implicitly handle contin...">GTransducer::supportedFeatureRange</a>.  <a href="#a5d699a8170b1c4bea6d2d1bbcc6d5395"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ac0f655fd9a14b02e23df3148d32844c3">beginIncrementalLearningInner</a> (<a class="el" href="class_g_classes_1_1smart__ptr.html">sp_relation</a> &amp;pFeatureRel, <a class="el" href="class_g_classes_1_1smart__ptr.html">sp_relation</a> &amp;pLabelRel)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_incremental_learner.html#a310f8acb279c2acc45999ae2acc32989" title="Prepare the model for incremental learning.">GIncrementalLearner::beginIncrementalLearningInner</a>.  <a href="#ac0f655fd9a14b02e23df3148d32844c3"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ad49ec97940b66ab5b6d190dda97f7395">trainIncrementalInner</a> (const double *pIn, const double *pOut)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">See the comment for <a class="el" href="class_g_classes_1_1_g_incremental_learner.html#a451f36cd03629d77e43244f051fe31ef" title="Refine the model with the specified pattern.">GIncrementalLearner::trainIncrementalInner</a>.  <a href="#ad49ec97940b66ab5b6d190dda97f7395"></a><br/></td></tr>
<tr><td colspan="2"><h2><a name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">std::vector&lt; <a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a92c3817e46a09eb23dc7e8b511c527e0">m_layers</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_back_prop.html">GBackProp</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#acc73a2f7b836adec6ce7fadcf15520f5">m_pBackProp</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a962c05723850b1b91a6e81f598fd9eac">m_internalFeatureDims</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a025e4b0408e8f6d41b787b984d6b8786">m_internalLabelDims</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">std::vector<br class="typebreak"/>
&lt; <a class="el" href="class_g_classes_1_1_g_activation_function.html">GActivationFunction</a> * &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ab2b1de8a28c9229f2fb42f072178b141">m_activationFunctions</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_activation_function.html">GActivationFunction</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a266df7e66c43efb5413edef8c1bd2356">m_pActivationFunction</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a860d67144206d666e8823554ad59e004">m_learningRate</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#aa91d9ff41ea8cf0bf9aebd5c38eaffc4">m_momentum</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#adf2792b6c57bc14e309f4d3f57bd259c">m_validationPortion</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a1373365847eba4087b4578a781455bd4">m_minImprovement</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a2a6f83fbb12e0a26a65093114646e57c">m_epochsPerValidationCheck</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a7ba6d45b553272733e346939220d7557">m_backPropTargetFunction</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#ad9e84eb276d162d57ecee68e49271bc2">m_useInputBias</a></td></tr>
<tr><td colspan="2"><h2><a name="friends"></a>
Friends</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">class&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_g_classes_1_1_g_neural_net.html#afd0d02db90315bd84b88cc5924a29053">GBackProp</a></td></tr>
</table>
<hr/><a name="_details"></a><h2>Detailed Description</h2>
<div class="textblock"><p>An artificial neural network. </p>
</div><hr/><h2>Member Enumeration Documentation</h2>
<a class="anchor" id="a3821c260c2cb0dbc64b6b0fbd0aabe84"></a><!-- doxytag: member="GClasses::GNeuralNet::TargetFunction" ref="a3821c260c2cb0dbc64b6b0fbd0aabe84" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">GClasses::GNeuralNet::TargetFunction</a></td>
        </tr>
      </table>
</div>
<div class="memdoc">
<dl><dt><b>Enumerator: </b></dt><dd><table border="0" cellspacing="2" cellpadding="0">
<tr><td valign="top"><em><a class="anchor" id="a3821c260c2cb0dbc64b6b0fbd0aabe84a6b9faea53545aa616825e2885f6816eb"></a><!-- doxytag: member="squared_error" ref="a3821c260c2cb0dbc64b6b0fbd0aabe84a6b9faea53545aa616825e2885f6816eb" args="" -->squared_error</em>&nbsp;</td><td>
</td></tr>
<tr><td valign="top"><em><a class="anchor" id="a3821c260c2cb0dbc64b6b0fbd0aabe84a9c238cea3a50aa79304e64ac443e991b"></a><!-- doxytag: member="cross_entropy" ref="a3821c260c2cb0dbc64b6b0fbd0aabe84a9c238cea3a50aa79304e64ac443e991b" args="" -->cross_entropy</em>&nbsp;</td><td>
<p>(default) best for regression </p>
</td></tr>
<tr><td valign="top"><em><a class="anchor" id="a3821c260c2cb0dbc64b6b0fbd0aabe84a46c15a6073e266659036be96f12e2a69"></a><!-- doxytag: member="sign" ref="a3821c260c2cb0dbc64b6b0fbd0aabe84a46c15a6073e266659036be96f12e2a69" args="" -->sign</em>&nbsp;</td><td>
<p>best for classification </p>
</td></tr>
<tr><td valign="top"><em><a class="anchor" id="a3821c260c2cb0dbc64b6b0fbd0aabe84ac01a705fd048c4ecbc78b52f6fde378a"></a><!-- doxytag: member="physical" ref="a3821c260c2cb0dbc64b6b0fbd0aabe84ac01a705fd048c4ecbc78b52f6fde378a" args="" -->physical</em>&nbsp;</td><td>
<p>uses the sign of the error, as in the perceptron training rule </p>
</td></tr>
</table>
</dd>
</dl>

</div>
</div>
<hr/><h2>Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="a899899ef6ba0eb9526567322bee7c413"></a><!-- doxytag: member="GClasses::GNeuralNet::GNeuralNet" ref="a899899ef6ba0eb9526567322bee7c413" args="(GRand &amp;rand)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">GClasses::GNeuralNet::GNeuralNet </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_rand.html">GRand</a> &amp;&#160;</td>
          <td class="paramname"><em>rand</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a6e8cbe8626913a36df44f5a843c1ca36"></a><!-- doxytag: member="GClasses::GNeuralNet::GNeuralNet" ref="a6e8cbe8626913a36df44f5a843c1ca36" args="(GDomNode *pNode, GLearnerLoader &amp;ll)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">GClasses::GNeuralNet::GNeuralNet </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_dom_node.html">GDomNode</a> *&#160;</td>
          <td class="paramname"><em>pNode</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_learner_loader.html">GLearnerLoader</a> &amp;&#160;</td>
          <td class="paramname"><em>ll</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Load from a text-format. </p>

</div>
</div>
<a class="anchor" id="a090fea822a920c75c99389aae9b2ebc7"></a><!-- doxytag: member="GClasses::GNeuralNet::~GNeuralNet" ref="a090fea822a920c75c99389aae9b2ebc7" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual GClasses::GNeuralNet::~GNeuralNet </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<hr/><h2>Member Function Documentation</h2>
<a class="anchor" id="a269ad30a3d161a3069cf16ffdd6746d3"></a><!-- doxytag: member="GClasses::GNeuralNet::addLayer" ref="a269ad30a3d161a3069cf16ffdd6746d3" args="(size_t nNodes)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::addLayer </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>nNodes</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Adds a hidden layer to the network. (The first hidden layer that you add will be adjacent to the input features. The last hidden layer that you add will be adjacent to the output layer.) </p>

</div>
</div>
<a class="anchor" id="aecbaee8213cc4cdf64aff5842ac7f255"></a><!-- doxytag: member="GClasses::GNeuralNet::addNode" ref="aecbaee8213cc4cdf64aff5842ac7f255" args="(size_t layer)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::addNode </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>layer</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Adds a new node at the end of the specified layer. (The new node is initialized with small weights, so this operation should initially have little impact on predictions.) </p>

</div>
</div>
<a class="anchor" id="adeceb444d141cfa17e2e245c1f6f3419"></a><!-- doxytag: member="GClasses::GNeuralNet::align" ref="adeceb444d141cfa17e2e245c1f6f3419" args="(GNeuralNet &amp;that)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::align </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> &amp;&#160;</td>
          <td class="paramname"><em>that</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Swaps nodes in hidden layers of this neural network to align with those in that neural network, as determined using bipartite matching. (This might be done, for example, before averaging weights together.) </p>

</div>
</div>
<a class="anchor" id="a253b2d399a42a25753b361ff1e758053"></a><!-- doxytag: member="GClasses::GNeuralNet::autoTune" ref="a253b2d399a42a25753b361ff1e758053" args="(GMatrix &amp;features, GMatrix &amp;labels)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::autoTune </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>labels</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Uses cross-validation to find a set of parameters that works well with the provided data. That is, this method will add a good number of hidden layers, pick a good momentum value, etc. </p>

</div>
</div>
<a class="anchor" id="afde11cca4167200c72b6567ef03b50fa"></a><!-- doxytag: member="GClasses::GNeuralNet::backProp" ref="afde11cca4167200c72b6567ef03b50fa" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_back_prop.html">GBackProp</a>* GClasses::GNeuralNet::backProp </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns the backprop object associated with this neural net (if there is one) </p>

</div>
</div>
<a class="anchor" id="ad5187a12369a854003fc455a36a6284e"></a><!-- doxytag: member="GClasses::GNeuralNet::backPropTargetFunction" ref="ad5187a12369a854003fc455a36a6284e" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a> GClasses::GNeuralNet::backPropTargetFunction </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns the enumeration of the target function used for backpropagation. </p>

</div>
</div>
<a class="anchor" id="ac0f655fd9a14b02e23df3148d32844c3"></a><!-- doxytag: member="GClasses::GNeuralNet::beginIncrementalLearningInner" ref="ac0f655fd9a14b02e23df3148d32844c3" args="(sp_relation &amp;pFeatureRel, sp_relation &amp;pLabelRel)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GNeuralNet::beginIncrementalLearningInner </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1smart__ptr.html">sp_relation</a> &amp;&#160;</td>
          <td class="paramname"><em>pFeatureRel</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1smart__ptr.html">sp_relation</a> &amp;&#160;</td>
          <td class="paramname"><em>pLabelRel</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_incremental_learner.html#a310f8acb279c2acc45999ae2acc32989" title="Prepare the model for incremental learning.">GIncrementalLearner::beginIncrementalLearningInner</a>. </p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_incremental_learner.html#a310f8acb279c2acc45999ae2acc32989">GClasses::GIncrementalLearner</a>.</p>

</div>
</div>
<a class="anchor" id="ae82282648edc1a9a16f87e7b1ecadaa5"></a><!-- doxytag: member="GClasses::GNeuralNet::canImplicitlyHandleMissingFeatures" ref="ae82282648edc1a9a16f87e7b1ecadaa5" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool GClasses::GNeuralNet::canImplicitlyHandleMissingFeatures </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline, protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#ad4557a177f7d0c92f26d40c102418e90" title="Returns true iff this algorithm supports missing feature values. If it cannot, then an imputation fil...">GTransducer::canImplicitlyHandleMissingFeatures</a>. </p>

<p>Reimplemented from <a class="el" href="class_g_classes_1_1_g_transducer.html#ad4557a177f7d0c92f26d40c102418e90">GClasses::GTransducer</a>.</p>

</div>
</div>
<a class="anchor" id="a9507ad0abba2a4baafecc64f56422301"></a><!-- doxytag: member="GClasses::GNeuralNet::canImplicitlyHandleNominalFeatures" ref="a9507ad0abba2a4baafecc64f56422301" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool GClasses::GNeuralNet::canImplicitlyHandleNominalFeatures </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline, protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#a07c1bac223ab1fc1eb369a10313688de" title="Returns true iff this algorithm can implicitly handle nominal features. If it cannot, then the GNominalToCat transform will be used to convert nominal features to continuous values before passing them to it.">GTransducer::canImplicitlyHandleNominalFeatures</a>. </p>

<p>Reimplemented from <a class="el" href="class_g_classes_1_1_g_transducer.html#a07c1bac223ab1fc1eb369a10313688de">GClasses::GTransducer</a>.</p>

</div>
</div>
<a class="anchor" id="a6528bdaa8bfdfc8174237791f3a54238"></a><!-- doxytag: member="GClasses::GNeuralNet::canImplicitlyHandleNominalLabels" ref="a6528bdaa8bfdfc8174237791f3a54238" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool GClasses::GNeuralNet::canImplicitlyHandleNominalLabels </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline, protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#a5b07f5b4775bb69cb982d64f0b75c430" title="Returns true iff this algorithm can implicitly handle nominal labels (a.k.a. classification). If it cannot, then the GNominalToCat transform will be used during training to convert nominal labels to continuous values, and to convert categorical predictions back to nominal labels.">GTransducer::canImplicitlyHandleNominalLabels</a>. </p>

<p>Reimplemented from <a class="el" href="class_g_classes_1_1_g_transducer.html#a5b07f5b4775bb69cb982d64f0b75c430">GClasses::GTransducer</a>.</p>

</div>
</div>
<a class="anchor" id="a034d339a9946207f5cf20478c96ffdad"></a><!-- doxytag: member="GClasses::GNeuralNet::clear" ref="a034d339a9946207f5cf20478c96ffdad" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GNeuralNet::clear </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#aa4df3b300514838026aa3eda33315fb4" title="Discards all training for the purpose of freeing memory. If you call this method, you must train befo...">GSupervisedLearner::clear</a>. </p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#aa4df3b300514838026aa3eda33315fb4">GClasses::GSupervisedLearner</a>.</p>

</div>
</div>
<a class="anchor" id="a9ce8cc5daefb05c2fc60b2c2f1257952"></a><!-- doxytag: member="GClasses::GNeuralNet::clipWeights" ref="a9ce8cc5daefb05c2fc60b2c2f1257952" args="(double max)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::clipWeights </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>max</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Clips all non-bias weights to fall within the range [-max, max]. </p>

</div>
</div>
<a class="anchor" id="ae87bc9372fb5705cf3bde1481faef090"></a><!-- doxytag: member="GClasses::GNeuralNet::copyPrediction" ref="ae87bc9372fb5705cf3bde1481faef090" args="(double *pOut)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::copyPrediction </td>
          <td>(</td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pOut</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This method assumes forwardProp has been called. It copies the predicted vector into pOut. </p>

</div>
</div>
<a class="anchor" id="a93626ba454ef61f90b87df71a267b43e"></a><!-- doxytag: member="GClasses::GNeuralNet::copyStructure" ref="a93626ba454ef61f90b87df71a267b43e" args="(GNeuralNet *pOther)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::copyStructure </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *&#160;</td>
          <td class="paramname"><em>pOther</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Copies the layers, nodes, and settings from pOther (but not the weights). beginIncrementalLearning must have been called on pOther so that it has a complete structure. </p>

</div>
</div>
<a class="anchor" id="af2de389bfa8fc82fc4c4131b277b1f3b"></a><!-- doxytag: member="GClasses::GNeuralNet::copyWeights" ref="af2de389bfa8fc82fc4c4131b277b1f3b" args="(GNeuralNet *pOther)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::copyWeights </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a> *&#160;</td>
          <td class="paramname"><em>pOther</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Copy the weights from pOther. It is assumed (but not checked) that pOther has the same network structure as this neural network. </p>

</div>
</div>
<a class="anchor" id="a62993759833ca851fd776a5ac0baa2b9"></a><!-- doxytag: member="GClasses::GNeuralNet::countWeights" ref="a62993759833ca851fd776a5ac0baa2b9" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t GClasses::GNeuralNet::countWeights </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Counts the number of weights in the network. (This value is not cached, so you should cache it rather than frequently call this method.) </p>

</div>
</div>
<a class="anchor" id="abc535e04fa4d64fa2c641d3c1f5a058f"></a><!-- doxytag: member="GClasses::GNeuralNet::decayWeights" ref="abc535e04fa4d64fa2c641d3c1f5a058f" args="(double lambda, double gamma=1.0)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::decayWeights </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>lambda</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>gamma</em> = <code>1.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Multiplies all non-bias weights by (1.0 - (learning_rate * lambda)), starting with the output layer, and ending with the first hidden layer. Typical values for lambda are small (like 0.001.) After each layer, the value of lambda is multiplied by gamma. (If gamma is greater than 1.0, then weights in hidden layers will decay faster, and if gamma is less than 1.0, then weights in hidden layers will decay slower.) It may be significant to note that if a regularizing penalty is added to the error of lambda times the sum-squared values of non-bias weights, then on-line weight updating works out to the same as decaying the weights after each application of back-prop. </p>

</div>
</div>
<a class="anchor" id="a96d7b1f6807b1ccd1110c7e6650015b0"></a><!-- doxytag: member="GClasses::GNeuralNet::decayWeightsSingleOutput" ref="a96d7b1f6807b1ccd1110c7e6650015b0" args="(size_t output, double lambda)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::decayWeightsSingleOutput </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>lambda</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Just like decayWeights, except it only decays the weights in one of the output units. </p>

</div>
</div>
<a class="anchor" id="ad7aba05b75ee2d29d67bab3db14f5af7"></a><!-- doxytag: member="GClasses::GNeuralNet::dropNode" ref="ad7aba05b75ee2d29d67bab3db14f5af7" args="(size_t layer, size_t node)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::dropNode </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>layer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Removes the specified node from the specified layer. (An exception will be thrown the layer only has one node.) </p>

</div>
</div>
<a class="anchor" id="a0f5e4c4d72d08a8219b95b7e059ae7cc"></a><!-- doxytag: member="GClasses::GNeuralNet::forwardProp" ref="a0f5e4c4d72d08a8219b95b7e059ae7cc" args="(const double *pInputs)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::forwardProp </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pInputs</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Evaluates a feature vector. (The results will be in the nodes of the output layer.) </p>

</div>
</div>
<a class="anchor" id="a625914c24e4a7ca1bb35d80317e9f409"></a><!-- doxytag: member="GClasses::GNeuralNet::forwardPropSingleOutput" ref="a625914c24e4a7ca1bb35d80317e9f409" args="(const double *pInputs, size_t output)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double GClasses::GNeuralNet::forwardPropSingleOutput </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pInputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>output</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This is the same as forwardProp, except it only propagates to a single output node. It returns the value that this node outputs. </p>

</div>
</div>
<a class="anchor" id="aeb58b1ee21b56a3f38d532bce284e1b9"></a><!-- doxytag: member="GClasses::GNeuralNet::hasTrainingBegun" ref="aeb58b1ee21b56a3f38d532bce284e1b9" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool GClasses::GNeuralNet::hasTrainingBegun </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns true iff train or beginIncrementalTraining has been called. </p>

</div>
</div>
<a class="anchor" id="a1fef0a2e6706f31d1d47f6fdefa328f1"></a><!-- doxytag: member="GClasses::GNeuralNet::improvementThresh" ref="a1fef0a2e6706f31d1d47f6fdefa328f1" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double GClasses::GNeuralNet::improvementThresh </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns the threshold ratio for improvement. </p>

</div>
</div>
<a class="anchor" id="a4041182ca3c44c4c9cee771305518cee"></a><!-- doxytag: member="GClasses::GNeuralNet::internalTraininGMatrix" ref="a4041182ca3c44c4c9cee771305518cee" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a>* GClasses::GNeuralNet::internalTraininGMatrix </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Gets the internal training data set. </p>

</div>
</div>
<a class="anchor" id="a376fb711d15ec0ed160a64c3e4ccc249"></a><!-- doxytag: member="GClasses::GNeuralNet::internalValidationData" ref="a376fb711d15ec0ed160a64c3e4ccc249" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a>* GClasses::GNeuralNet::internalValidationData </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Gets the internal validation data set. </p>

</div>
</div>
<a class="anchor" id="ad70a9ed88aca050141139babb790f516"></a><!-- doxytag: member="GClasses::GNeuralNet::layer" ref="ad70a9ed88aca050141139babb790f516" args="(size_t n)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a>&amp; GClasses::GNeuralNet::layer </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>n</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns a reference to the specified layer. </p>

</div>
</div>
<a class="anchor" id="a449d358ad9956d5fc61db21efe1a3942"></a><!-- doxytag: member="GClasses::GNeuralNet::layerCount" ref="a449d358ad9956d5fc61db21efe1a3942" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t GClasses::GNeuralNet::layerCount </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns the number of layers in this neural network. (Every network has at least one output layer, plus all of the hidden layers that you add by calling addLayer. The input vector does not count as a layer, even though it may be common to visualize it as a layer.) </p>

</div>
</div>
<a class="anchor" id="a40d6982c5565bdb16711ab0361471614"></a><!-- doxytag: member="GClasses::GNeuralNet::learningRate" ref="a40d6982c5565bdb16711ab0361471614" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double GClasses::GNeuralNet::learningRate </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns the current learning rate. </p>

</div>
</div>
<a class="anchor" id="aaae62462d9be256a2a260c17eef4afbc"></a><!-- doxytag: member="GClasses::GNeuralNet::momentum" ref="aaae62462d9be256a2a260c17eef4afbc" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double GClasses::GNeuralNet::momentum </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns the current momentum value. </p>

</div>
</div>
<a class="anchor" id="a85534d9115c3ab715d3043715e6c2145"></a><!-- doxytag: member="GClasses::GNeuralNet::perturbAllWeights" ref="a85534d9115c3ab715d3043715e6c2145" args="(double deviation)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::perturbAllWeights </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>deviation</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Perturbs all weights in the network by a random normal offset with the specified deviation. </p>

</div>
</div>
<a class="anchor" id="a98495c7bfb4fa4cb68d36dafd2428fac"></a><!-- doxytag: member="GClasses::GNeuralNet::predictDistributionInner" ref="a98495c7bfb4fa4cb68d36dafd2428fac" args="(const double *pIn, GPrediction *pOut)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GNeuralNet::predictDistributionInner </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pIn</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_prediction.html">GPrediction</a> *&#160;</td>
          <td class="paramname"><em>pOut</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#a78853095b1557cdd6ceea9814f8a0a7a" title="This is the implementation of the model&#39;s prediction algorithm. (This method is called by predict...">GSupervisedLearner::predictDistributionInner</a>. </p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#a78853095b1557cdd6ceea9814f8a0a7a">GClasses::GSupervisedLearner</a>.</p>

</div>
</div>
<a class="anchor" id="aa201d1358f827511fc31389f4e8cedcc"></a><!-- doxytag: member="GClasses::GNeuralNet::predictInner" ref="aa201d1358f827511fc31389f4e8cedcc" args="(const double *pIn, double *pOut)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GNeuralNet::predictInner </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pIn</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pOut</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#a4fec9ccd13a7cf09c87762f3073cba79" title="This is the implementation of the model&#39;s prediction algorithm. (This method is called by predict...">GSupervisedLearner::predictInner</a>. </p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#a4fec9ccd13a7cf09c87762f3073cba79">GClasses::GSupervisedLearner</a>.</p>

</div>
</div>
<a class="anchor" id="aed2999a007c3fcf02e484e9d87c627d5"></a><!-- doxytag: member="GClasses::GNeuralNet::releaseTrainingJunk" ref="aed2999a007c3fcf02e484e9d87c627d5" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::releaseTrainingJunk </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Some extra junk is allocated when training to make it efficient. This method is called when training is done to get rid of that extra junk. </p>

</div>
</div>
<a class="anchor" id="a1d36eb292d21354a5e14b82f3359fb4e"></a><!-- doxytag: member="GClasses::GNeuralNet::serialize" ref="a1d36eb292d21354a5e14b82f3359fb4e" args="(GDom *pDoc)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual <a class="el" href="class_g_classes_1_1_g_dom_node.html">GDomNode</a>* GClasses::GNeuralNet::serialize </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_dom.html">GDom</a> *&#160;</td>
          <td class="paramname"><em>pDoc</em></td><td>)</td>
          <td><code> [virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Saves the model to a text file. </p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#a5f12bfb94c9d74b69fb55b29fe787e8c">GClasses::GSupervisedLearner</a>.</p>

</div>
</div>
<a class="anchor" id="a3abb685af4764fa0f0c34492799f91b1"></a><!-- doxytag: member="GClasses::GNeuralNet::setActivationFunction" ref="a3abb685af4764fa0f0c34492799f91b1" args="(GActivationFunction *pSF, bool hold)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setActivationFunction </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_activation_function.html">GActivationFunction</a> *&#160;</td>
          <td class="paramname"><em>pSF</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>hold</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Sets the activation function to use with all subsequently added layers. (Note that the activation function for the output layer is set when train or beginIncrementalLearning is called, so if you only wish to set the squshing function for the output layer, call this method after all hidden layers have been added, but before you call train.) If hold is true, then the neural network will hold on to this instance of the activation function and delete it when the neural network is deleted. </p>

</div>
</div>
<a class="anchor" id="a3942e81858018f7977e97a99fd222254"></a><!-- doxytag: member="GClasses::GNeuralNet::setBackPropTargetFunction" ref="a3942e81858018f7977e97a99fd222254" args="(TargetFunction eTF)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setBackPropTargetFunction </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a>&#160;</td>
          <td class="paramname"><em>eTF</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Specify the target function to use for back-propagation. The default is squared_error. cross_entropy tends to be faster, and is well-suited for classification tasks. </p>

</div>
</div>
<a class="anchor" id="a2d02f321a6232f3b1640504c235655c9"></a><!-- doxytag: member="GClasses::GNeuralNet::setErrorOnOutputLayer" ref="a2d02f321a6232f3b1640504c235655c9" args="(const double *pTarget, TargetFunction eTargetFunction=squared_error)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setErrorOnOutputLayer </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pTarget</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a>&#160;</td>
          <td class="paramname"><em>eTargetFunction</em> = <code>squared_error</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This method assumes that forwardProp has already been called. (Note that the predict method calls forwardProp). It computes the error values at each node in the output layer. After calling this method, it is typical to call <a class="el" href="class_g_classes_1_1_g_neural_net.html#afde11cca4167200c72b6567ef03b50fa" title="Returns the backprop object associated with this neural net (if there is one)">backProp()</a>-&gt;backpropagate(), to compute the error on the hidden nodes, and then to call <a class="el" href="class_g_classes_1_1_g_neural_net.html#afde11cca4167200c72b6567ef03b50fa" title="Returns the backprop object associated with this neural net (if there is one)">backProp()</a>-&gt;descendGradient to update the weights. pTarget contains the target values for the ouptut nodes. </p>

</div>
</div>
<a class="anchor" id="ac19924e551a016c0916ae4ba561ea9ce"></a><!-- doxytag: member="GClasses::GNeuralNet::setErrorSingleOutput" ref="ac19924e551a016c0916ae4ba561ea9ce" args="(double target, size_t output, TargetFunction eTargetFunction=squared_error)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setErrorSingleOutput </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>target</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a>&#160;</td>
          <td class="paramname"><em>eTargetFunction</em> = <code>squared_error</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This is teh same as setErrorOnOutputLayer, except that it only sets the error on a single output node. </p>

</div>
</div>
<a class="anchor" id="ac12aa4d7571dcb380ee03da2a1634854"></a><!-- doxytag: member="GClasses::GNeuralNet::setImprovementThresh" ref="ac12aa4d7571dcb380ee03da2a1634854" args="(double d)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setImprovementThresh </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>d</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Specifies the threshold ratio for improvement that must be made since the last validation check for training to continue. (For example, if the mean squared error at the previous validation check was 50, and the mean squared error at the current validation check is 49, then training will stop if d is &gt; 0.02.) </p>

</div>
</div>
<a class="anchor" id="abfdf6d4186899d5f508d32a336903594"></a><!-- doxytag: member="GClasses::GNeuralNet::setLearningRate" ref="abfdf6d4186899d5f508d32a336903594" args="(double d)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setLearningRate </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>d</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the learning rate. </p>

</div>
</div>
<a class="anchor" id="a3294a63458679248ed777c433ae3c66d"></a><!-- doxytag: member="GClasses::GNeuralNet::setMomentum" ref="a3294a63458679248ed777c433ae3c66d" args="(double d)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setMomentum </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>d</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Momentum has the effect of speeding convergence and helping the gradient descent algorithm move past some local minimums. </p>

</div>
</div>
<a class="anchor" id="a201f1bb4bca2d429938dec193a2af973"></a><!-- doxytag: member="GClasses::GNeuralNet::setUseInputBias" ref="a201f1bb4bca2d429938dec193a2af973" args="(bool b)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setUseInputBias </td>
          <td>(</td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>b</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Specify whether to use an input bias. (The default is false.) This feature is used with generative-backpropagation, which adjusts inputs to create latent features. </p>

</div>
</div>
<a class="anchor" id="a0f3e41a0e0e9e363e2398de2ba231f33"></a><!-- doxytag: member="GClasses::GNeuralNet::setValidationPortion" ref="a0f3e41a0e0e9e363e2398de2ba231f33" args="(double d)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setValidationPortion </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>d</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the portion of the data that will be used for validation. If the value is 0, then all of the data is used for both training and validation. </p>

</div>
</div>
<a class="anchor" id="ac7b2fc220e96389fc0cb4d3694d40a02"></a><!-- doxytag: member="GClasses::GNeuralNet::setWeights" ref="ac7b2fc220e96389fc0cb4d3694d40a02" args="(const double *pWeights)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setWeights </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pWeights</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Sets all the weights from an array of doubles. The number of doubles in the array can be determined by calling <a class="el" href="class_g_classes_1_1_g_neural_net.html#a62993759833ca851fd776a5ac0baa2b9" title="Counts the number of weights in the network. (This value is not cached, so you should cache it rather...">countWeights()</a>. </p>

</div>
</div>
<a class="anchor" id="add517c7b14f72ce910e10f9e3eb9ae38"></a><!-- doxytag: member="GClasses::GNeuralNet::setWindowSize" ref="add517c7b14f72ce910e10f9e3eb9ae38" args="(size_t n)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::setWindowSize </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>n</em></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Sets the number of epochs that will be performed before each time the network is tested again with the validation set to determine if we have a better best-set of weights, and whether or not it's achieved the termination condition yet. (An epochs is defined as a single pass through all rows in the training set.) </p>

</div>
</div>
<a class="anchor" id="a4fc5b7d1e52af80cd3676a759e1bccc2"></a><!-- doxytag: member="GClasses::GNeuralNet::sumSquaredPredictionError" ref="a4fc5b7d1e52af80cd3676a759e1bccc2" args="(const double *pTarget)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double GClasses::GNeuralNet::sumSquaredPredictionError </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pTarget</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This method assumes forwardProp has been called. It computes the sum squared prediction error with the specified target vector. </p>

</div>
</div>
<a class="anchor" id="a18ca749935c5a40ce3a9e9ae138196ff"></a><!-- doxytag: member="GClasses::GNeuralNet::supportedFeatureRange" ref="a18ca749935c5a40ce3a9e9ae138196ff" args="(double *pOutMin, double *pOutMax)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool GClasses::GNeuralNet::supportedFeatureRange </td>
          <td>(</td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pOutMin</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pOutMax</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#ac7b2855e39b2897801641b42870dee20" title="Returns true if this algorithm supports any feature value, or if it does not implicitly handle contin...">GTransducer::supportedFeatureRange</a>. </p>

<p>Reimplemented from <a class="el" href="class_g_classes_1_1_g_transducer.html#ac7b2855e39b2897801641b42870dee20">GClasses::GTransducer</a>.</p>

</div>
</div>
<a class="anchor" id="a5d699a8170b1c4bea6d2d1bbcc6d5395"></a><!-- doxytag: member="GClasses::GNeuralNet::supportedLabelRange" ref="a5d699a8170b1c4bea6d2d1bbcc6d5395" args="(double *pOutMin, double *pOutMax)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool GClasses::GNeuralNet::supportedLabelRange </td>
          <td>(</td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pOutMin</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pOutMax</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_transducer.html#ac7b2855e39b2897801641b42870dee20" title="Returns true if this algorithm supports any feature value, or if it does not implicitly handle contin...">GTransducer::supportedFeatureRange</a>. </p>

<p>Reimplemented from <a class="el" href="class_g_classes_1_1_g_transducer.html#a477f7104321653ab2a089f273c5002ee">GClasses::GTransducer</a>.</p>

</div>
</div>
<a class="anchor" id="a93ccc4e04cc4d70f6451f4eb5171b9f3"></a><!-- doxytag: member="GClasses::GNeuralNet::swapNodes" ref="a93ccc4e04cc4d70f6451f4eb5171b9f3" args="(size_t layer, size_t a, size_t b)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::swapNodes </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>layer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>b</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Swaps two nodes in the specified layer. If layer specifies one of the hidden layers, then this will have no net effect on the output of the network. (Assumes this model is already trained.) </p>

</div>
</div>
<a class="anchor" id="ab0b1fe168491730483217ccccf6cf374"></a><!-- doxytag: member="GClasses::GNeuralNet::test" ref="ab0b1fe168491730483217ccccf6cf374" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">static void GClasses::GNeuralNet::test </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Performs unit tests for this class. Throws an exception if there is a failure. </p>

<p>Reimplemented from <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#a2882bcbe9c900d797641ad9e45385df2">GClasses::GSupervisedLearner</a>.</p>

</div>
</div>
<a class="anchor" id="ad49ec97940b66ab5b6d190dda97f7395"></a><!-- doxytag: member="GClasses::GNeuralNet::trainIncrementalInner" ref="ad49ec97940b66ab5b6d190dda97f7395" args="(const double *pIn, const double *pOut)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GNeuralNet::trainIncrementalInner </td>
          <td>(</td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pIn</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double *&#160;</td>
          <td class="paramname"><em>pOut</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_incremental_learner.html#a451f36cd03629d77e43244f051fe31ef" title="Refine the model with the specified pattern.">GIncrementalLearner::trainIncrementalInner</a>. </p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_incremental_learner.html#a451f36cd03629d77e43244f051fe31ef">GClasses::GIncrementalLearner</a>.</p>

</div>
</div>
<a class="anchor" id="a903d83e71d80899a49c816769f54d360"></a><!-- doxytag: member="GClasses::GNeuralNet::trainInner" ref="a903d83e71d80899a49c816769f54d360" args="(GMatrix &amp;features, GMatrix &amp;labels)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GNeuralNet::trainInner </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>labels</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [protected, virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#aa613836bbc260ba1665c7bfa3f5c5a6b" title="This is the implementation of the model&#39;s training algorithm. (This method is called by train)...">GSupervisedLearner::trainInner</a>. </p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_supervised_learner.html#aa613836bbc260ba1665c7bfa3f5c5a6b">GClasses::GSupervisedLearner</a>.</p>

</div>
</div>
<a class="anchor" id="a3c726ba4f3a5924dd0cf4130b27529d9"></a><!-- doxytag: member="GClasses::GNeuralNet::trainSparse" ref="a3c726ba4f3a5924dd0cf4130b27529d9" args="(GSparseMatrix &amp;features, GMatrix &amp;labels)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">virtual void GClasses::GNeuralNet::trainSparse </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_sparse_matrix.html">GSparseMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>labels</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>See the comment for <a class="el" href="class_g_classes_1_1_g_incremental_learner.html#a25ddbd1ac0eb49543f8a752585082c50" title="Train using a sparse feature matrix. (A Typical implementation of this method will first call beginIn...">GIncrementalLearner::trainSparse</a> Assumes all attributes are continuous. </p>

<p>Implements <a class="el" href="class_g_classes_1_1_g_incremental_learner.html#a25ddbd1ac0eb49543f8a752585082c50">GClasses::GIncrementalLearner</a>.</p>

</div>
</div>
<a class="anchor" id="a6966d563836d90cb814ec1d69e0e2b06"></a><!-- doxytag: member="GClasses::GNeuralNet::trainWithValidation" ref="a6966d563836d90cb814ec1d69e0e2b06" args="(GMatrix &amp;trainFeatures, GMatrix &amp;trainLabels, GMatrix &amp;validateFeatures, GMatrix &amp;validateLabels)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t GClasses::GNeuralNet::trainWithValidation </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>trainFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>trainLabels</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>validateFeatures</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>validateLabels</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Train the network until the termination condition is met. Returns the number of epochs required to train it. </p>

</div>
</div>
<a class="anchor" id="ad307984e3c04ecce690d3675a19cb735"></a><!-- doxytag: member="GClasses::GNeuralNet::useInputBias" ref="ad307984e3c04ecce690d3675a19cb735" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool GClasses::GNeuralNet::useInputBias </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns whether this neural network utilizes an input bias. </p>

</div>
</div>
<a class="anchor" id="a7f38fd2cac389cb0c96cd215607c1a04"></a><!-- doxytag: member="GClasses::GNeuralNet::validationSquaredError" ref="a7f38fd2cac389cb0c96cd215607c1a04" args="(GMatrix &amp;features, GMatrix &amp;labels)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double GClasses::GNeuralNet::validationSquaredError </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_g_classes_1_1_g_matrix.html">GMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>labels</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Measures the sum squared error against the specified dataset. </p>

</div>
</div>
<a class="anchor" id="a48f738fa5bc10067b2901cc12624e3bb"></a><!-- doxytag: member="GClasses::GNeuralNet::weights" ref="a48f738fa5bc10067b2901cc12624e3bb" args="(double *pOutWeights)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void GClasses::GNeuralNet::weights </td>
          <td>(</td>
          <td class="paramtype">double *&#160;</td>
          <td class="paramname"><em>pOutWeights</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Serializes the network weights into an array of doubles. The number of doubles in the array can be determined by calling <a class="el" href="class_g_classes_1_1_g_neural_net.html#a62993759833ca851fd776a5ac0baa2b9" title="Counts the number of weights in the network. (This value is not cached, so you should cache it rather...">countWeights()</a>. </p>

</div>
</div>
<a class="anchor" id="a30752f8ebc861f868d8d539a8a41399a"></a><!-- doxytag: member="GClasses::GNeuralNet::windowSize" ref="a30752f8ebc861f868d8d539a8a41399a" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t GClasses::GNeuralNet::windowSize </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Returns the number of epochs to perform before the validation data is evaluated to see if training should stop. </p>

</div>
</div>
<hr/><h2>Friends And Related Function Documentation</h2>
<a class="anchor" id="afd0d02db90315bd84b88cc5924a29053"></a><!-- doxytag: member="GClasses::GNeuralNet::GBackProp" ref="afd0d02db90315bd84b88cc5924a29053" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">friend class <a class="el" href="class_g_classes_1_1_g_back_prop.html">GBackProp</a><code> [friend]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<hr/><h2>Member Data Documentation</h2>
<a class="anchor" id="ab2b1de8a28c9229f2fb42f072178b141"></a><!-- doxytag: member="GClasses::GNeuralNet::m_activationFunctions" ref="ab2b1de8a28c9229f2fb42f072178b141" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;<a class="el" href="class_g_classes_1_1_g_activation_function.html">GActivationFunction</a>*&gt; <a class="el" href="class_g_classes_1_1_g_neural_net.html#ab2b1de8a28c9229f2fb42f072178b141">GClasses::GNeuralNet::m_activationFunctions</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a7ba6d45b553272733e346939220d7557"></a><!-- doxytag: member="GClasses::GNeuralNet::m_backPropTargetFunction" ref="a7ba6d45b553272733e346939220d7557" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_neural_net.html#a3821c260c2cb0dbc64b6b0fbd0aabe84">TargetFunction</a> <a class="el" href="class_g_classes_1_1_g_neural_net.html#a7ba6d45b553272733e346939220d7557">GClasses::GNeuralNet::m_backPropTargetFunction</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a2a6f83fbb12e0a26a65093114646e57c"></a><!-- doxytag: member="GClasses::GNeuralNet::m_epochsPerValidationCheck" ref="a2a6f83fbb12e0a26a65093114646e57c" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t <a class="el" href="class_g_classes_1_1_g_neural_net.html#a2a6f83fbb12e0a26a65093114646e57c">GClasses::GNeuralNet::m_epochsPerValidationCheck</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a962c05723850b1b91a6e81f598fd9eac"></a><!-- doxytag: member="GClasses::GNeuralNet::m_internalFeatureDims" ref="a962c05723850b1b91a6e81f598fd9eac" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t <a class="el" href="class_g_classes_1_1_g_neural_net.html#a962c05723850b1b91a6e81f598fd9eac">GClasses::GNeuralNet::m_internalFeatureDims</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a025e4b0408e8f6d41b787b984d6b8786"></a><!-- doxytag: member="GClasses::GNeuralNet::m_internalLabelDims" ref="a025e4b0408e8f6d41b787b984d6b8786" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t <a class="el" href="class_g_classes_1_1_g_neural_net.html#a025e4b0408e8f6d41b787b984d6b8786">GClasses::GNeuralNet::m_internalLabelDims</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a92c3817e46a09eb23dc7e8b511c527e0"></a><!-- doxytag: member="GClasses::GNeuralNet::m_layers" ref="a92c3817e46a09eb23dc7e8b511c527e0" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;<a class="el" href="class_g_classes_1_1_g_neural_net_layer.html">GNeuralNetLayer</a>&gt; <a class="el" href="class_g_classes_1_1_g_neural_net.html#a92c3817e46a09eb23dc7e8b511c527e0">GClasses::GNeuralNet::m_layers</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a860d67144206d666e8823554ad59e004"></a><!-- doxytag: member="GClasses::GNeuralNet::m_learningRate" ref="a860d67144206d666e8823554ad59e004" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="class_g_classes_1_1_g_neural_net.html#a860d67144206d666e8823554ad59e004">GClasses::GNeuralNet::m_learningRate</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a1373365847eba4087b4578a781455bd4"></a><!-- doxytag: member="GClasses::GNeuralNet::m_minImprovement" ref="a1373365847eba4087b4578a781455bd4" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="class_g_classes_1_1_g_neural_net.html#a1373365847eba4087b4578a781455bd4">GClasses::GNeuralNet::m_minImprovement</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="aa91d9ff41ea8cf0bf9aebd5c38eaffc4"></a><!-- doxytag: member="GClasses::GNeuralNet::m_momentum" ref="aa91d9ff41ea8cf0bf9aebd5c38eaffc4" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="class_g_classes_1_1_g_neural_net.html#aa91d9ff41ea8cf0bf9aebd5c38eaffc4">GClasses::GNeuralNet::m_momentum</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="a266df7e66c43efb5413edef8c1bd2356"></a><!-- doxytag: member="GClasses::GNeuralNet::m_pActivationFunction" ref="a266df7e66c43efb5413edef8c1bd2356" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_activation_function.html">GActivationFunction</a>* <a class="el" href="class_g_classes_1_1_g_neural_net.html#a266df7e66c43efb5413edef8c1bd2356">GClasses::GNeuralNet::m_pActivationFunction</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="acc73a2f7b836adec6ce7fadcf15520f5"></a><!-- doxytag: member="GClasses::GNeuralNet::m_pBackProp" ref="acc73a2f7b836adec6ce7fadcf15520f5" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_g_classes_1_1_g_back_prop.html">GBackProp</a>* <a class="el" href="class_g_classes_1_1_g_neural_net.html#acc73a2f7b836adec6ce7fadcf15520f5">GClasses::GNeuralNet::m_pBackProp</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="ad9e84eb276d162d57ecee68e49271bc2"></a><!-- doxytag: member="GClasses::GNeuralNet::m_useInputBias" ref="ad9e84eb276d162d57ecee68e49271bc2" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool <a class="el" href="class_g_classes_1_1_g_neural_net.html#ad9e84eb276d162d57ecee68e49271bc2">GClasses::GNeuralNet::m_useInputBias</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
<a class="anchor" id="adf2792b6c57bc14e309f4d3f57bd259c"></a><!-- doxytag: member="GClasses::GNeuralNet::m_validationPortion" ref="adf2792b6c57bc14e309f4d3f57bd259c" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double <a class="el" href="class_g_classes_1_1_g_neural_net.html#adf2792b6c57bc14e309f4d3f57bd259c">GClasses::GNeuralNet::m_validationPortion</a><code> [protected]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

</div>
</div>
</div>
</div>
  <div id="nav-path" class="navpath">
    <ul>
      <li class="navelem"><a class="el" href="namespace_g_classes.html">GClasses</a>      </li>
      <li class="navelem"><a class="el" href="class_g_classes_1_1_g_neural_net.html">GNeuralNet</a>      </li>
      <li class="footer">Generated on Mon Dec 5 2011 14:19:02 for GClasses by&#160;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.7.3 </li>
    </ul>
  </div>

</body>
</html>
